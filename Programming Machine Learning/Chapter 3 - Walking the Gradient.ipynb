{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Walking the Gradient\n",
    "## Our Algorithm Doesn't Cut It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most problems are too complicated to be solved by a simple line with 2 parameters.\n",
    "- Adding more parameters to our train() function would kill its performance\n",
    "- In each iteration, the train() algorithm tweaks either w or b, but this can go wrong (tweaking w might increase the loss caused by b). \n",
    "    - To avoid this problem, we should tweak w and b at the same time. But, if there are many parameters (we could have hundreds of thousands of parameters), that would cause too many combinations to use this algorithm.\n",
    "- Also, using a small lr gives us high precision at the cost of too much time: we need both speed and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a better train() algorithm. The point of the function is to find the parameters that give you the lowest loss. \n",
    "\n",
    "Imagine the loss curve (page 34). We want to find w that gives us the lowest point of the curve. To code this, we need to find the slope (gradient) of the curve. \n",
    "\n",
    "Need to find \"the derivative of the loss with respect to the weight\", or ∂L/∂w.\n",
    "\n",
    "If the hiker is on the left, the derivative is negative, because the loss decreases as w increases\n",
    "\n",
    "If the hiker is on the right, the derivative is positive, because the loss increases as w increases.\n",
    "\n",
    "At the bottom of the curve, the curve is level and the derivative is zero.\n",
    "\n",
    "You have to walk in the opposite direction of the derivative to approach the minimum. So, if derivative is negative, you have to walk in the positive direction. \n",
    "\n",
    "Also, the size of the step should be proportional to derivative: if the derivative is a big number (either positive or negative), the curve is steep and the basecamp is far away. So, you can take big steps. As you approach the minimum, the derivative becomes smaller, and so do your steps.\n",
    "\n",
    "### A Sprinkle of Math\n",
    "\n",
    "\\begin{equation*}\n",
    "L = {1/m}( \\sum_{i=1}^m ((wx_i + b) - y_i)^2)\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Just Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On: Basecamp Overshooting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
